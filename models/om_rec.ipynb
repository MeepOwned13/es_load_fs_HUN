{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "174ec8f524d1b193"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-28T09:54:47.961910800Z",
     "start_time": "2023-11-28T09:54:43.020474400Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import trainer_lib as tl\n",
    "import torch_model_definitions as tmd\n",
    "\n",
    "torch.manual_seed(310231551)\n",
    "random.seed(3009231410)\n",
    "np.random.seed(2909231846)\n",
    "np_random_state = np.random.RandomState(131002)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "587b63899b1298c9"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df: pd.DataFrame = tl.load_country_wide_dataset('../data/country_data.csv')\n",
    "\n",
    "X = df.to_numpy(dtype=np.float32)\n",
    "y = X.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T09:54:48.242013300Z",
     "start_time": "2023-11-28T09:54:47.964912200Z"
    }
   },
   "id": "b483975330fa7452"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1241fd1ff050dbf"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class LSTMRec(nn.Module):\n",
    "    def __init__(self, features=11, hidden_size=20, num_layers=2, dropout=0.0, hid_noise=0.0, bidirectional=True, **kwargs):\n",
    "        super(LSTMRec, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.h_n_dim = 2 if bidirectional else 1\n",
    "        self.num_layers = num_layers\n",
    "        rec_drop = dropout if num_layers > 1 else 0.0\n",
    "        self.lstm = nn.LSTM(input_size=features, hidden_size=self.hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, dropout=rec_drop)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            tmd.GaussianNoise(hid_noise),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden_size * self.h_n_dim * self.num_layers, features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h_0 = torch.zeros(self.h_n_dim * self.num_layers, batch_size, self.hidden_size).to(tl.TRAINER_LIB_DEVICE)\n",
    "        c_0 = torch.zeros(self.h_n_dim * self.num_layers, batch_size, self.hidden_size).to(tl.TRAINER_LIB_DEVICE)\n",
    "\n",
    "        _, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        h_n = torch.permute(h_n, (1, 0, 2))\n",
    "        return self.fc(h_n)\n",
    "    \n",
    "class GRURec(nn.Module):\n",
    "    def __init__(self, features=11, hidden_size=20, num_layers=2, dropout=0.0, hid_noise=0.0, bidirectional=True, **kwargs):\n",
    "        super(GRURec, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.h_n_dim = 2 if bidirectional else 1\n",
    "        self.num_layers = num_layers\n",
    "        rec_drop = dropout if num_layers > 1 else 0.0\n",
    "        self.gru = nn.GRU(input_size=features, hidden_size=self.hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, dropout=rec_drop)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            tmd.GaussianNoise(hid_noise),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden_size * self.h_n_dim * self.num_layers, features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        hidden = torch.zeros(self.h_n_dim * self.num_layers, batch_size, self.hidden_size).to(tl.TRAINER_LIB_DEVICE)\n",
    "        \n",
    "        _, hidden = self.gru(x, hidden)\n",
    "        x = torch.permute(hidden, (1, 0, 2))\n",
    "        return self.fc(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T09:54:48.254837200Z",
     "start_time": "2023-11-28T09:54:48.250821300Z"
    }
   },
   "id": "e8e521f357fcaa3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grid search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "188b9f01ce6fa5b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid = tl.Grid({\n",
    "    'epochs': [1000],  # we use early stopping, so this is just a high number\n",
    "    'lr': [0.001],\n",
    "    'model': [LSTMRec, GRURec],\n",
    "    'hidden_size': [20, 40],\n",
    "    'num_layers': [2, 3],\n",
    "    'bidirectional': [True],\n",
    "    'batch_size': [2048],\n",
    "    'dropout': [0.5],\n",
    "    'hid_noise': [0.05],\n",
    "}) # val_mod is default at 8, n_splits at 6\n",
    "\n",
    "wrapper = tl.RECOneModelTSWrapper(LSTMRec(), seq_len=24, pred_len=3)\n",
    "b_p, b_s = wrapper.grid_search(X, y, grid, verbose=3)\n",
    "print(f\"\\nBest params: {b_p}\\nBest score: {b_s}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77d4f9ed316d4f71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The highest layer number and hidden size were the best, I will test even higher ones.\n",
    "LSTM and GRU trade blows in this comparison, further testing is required."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3480b94e7206252b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid = tl.Grid({\n",
    "    'epochs': [1000],  # we use early stopping, so this is just a high number\n",
    "    'lr': [0.001],\n",
    "    'model': [LSTMRec, GRURec],\n",
    "    'hidden_size': [60],\n",
    "    'num_layers': [3, 4],\n",
    "    'bidirectional': [True],\n",
    "    'batch_size': [2048],\n",
    "    'dropout': [0.5],\n",
    "    'hid_noise': [0.05],\n",
    "}) # val_mod is default at 8, n_splits at 6\n",
    "\n",
    "wrapper = tl.RECOneModelTSWrapper(LSTMRec(), seq_len=24, pred_len=3)\n",
    "b_p, b_s = wrapper.grid_search(X, y, grid, verbose=4)\n",
    "print(f\"\\nBest params: {b_p}\\nBest score: {b_s}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d993f4179a173103"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid = tl.Grid({\n",
    "    'epochs': [1000],  # we use early stopping, so this is just a high number\n",
    "    'lr': [0.0005],\n",
    "    'model': [GRURec],\n",
    "    'hidden_size': [50, 60, 70],\n",
    "    'num_layers': [4],\n",
    "    'bidirectional': [True],\n",
    "    'batch_size': [2048],\n",
    "    'dropout': [0.5],\n",
    "    'hid_noise': [0.05],\n",
    "}) # val_mod is default at 8, n_splits at 6\n",
    "\n",
    "wrapper = tl.RECOneModelTSWrapper(GRURec(), seq_len=24, pred_len=3)\n",
    "b_p, b_s = wrapper.grid_search(X, y, grid, verbose=3)\n",
    "print(f\"\\nBest params: {b_p}\\nBest score: {b_s}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ac240a89bb0551a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The GRU model with hidden size 70 and 4 layers seem to work the best."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5bce66cbffb6f7d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid = tl.Grid({\n",
    "    'epochs': [1000],  # we use early stopping, so this is just a high number\n",
    "    'lr': [0.001],\n",
    "    'model': [GRURec],\n",
    "    'hidden_size': [70, 80],\n",
    "    'num_layers': [5],\n",
    "    'bidirectional': [True],\n",
    "    'batch_size': [2048],\n",
    "    'dropout': [0.5],\n",
    "    'hid_noise': [0.05],\n",
    "}) # val_mod is default at 8, n_splits at 6\n",
    "\n",
    "wrapper = tl.RECOneModelTSWrapper(GRURec(), seq_len=24, pred_len=3)\n",
    "b_p, b_s = wrapper.grid_search(X, y, grid, verbose=3)\n",
    "print(f\"\\nBest params: {b_p}\\nBest score: {b_s}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "807a2f7e20ebf38b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Final"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e41b1d2e916b5e84"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] BEGIN\n",
      "\tEpoch 017: train loss: 0.235491, val loss: 0.372250, test loss: 0.728472  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m wrapper \u001B[38;5;241m=\u001B[39m tl\u001B[38;5;241m.\u001B[39mRECOneModelTSWrapper(GRURec(\u001B[38;5;241m11\u001B[39m, \u001B[38;5;241m70\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.05\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m), seq_len\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m24\u001B[39m, pred_len\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mwrapper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_ts_strategy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2048\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.001\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m6\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\Python\\TimeSeries\\es_load_fs_HUN\\models\\trainer_lib.py:476\u001B[0m, in \u001B[0;36mTSMWrapper.validate_ts_strategy\u001B[1;34m(self, x, y, epochs, loss_fn, val_mod, lr, batch_size, es_p, es_d, n_splits, verbose, cp, **kwargs)\u001B[0m\n\u001B[0;32m    473\u001B[0m y_train, y_val, y_test \u001B[38;5;241m=\u001B[39m y[train_idxs], y[val_idxs], y[test_idxs]\n\u001B[0;32m    475\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minit_strategy()\n\u001B[1;32m--> 476\u001B[0m train_loss, val_loss, test_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_strategy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    477\u001B[0m \u001B[43m                                                      \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    478\u001B[0m \u001B[43m                                                      \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mes_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mes_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mes_d\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mes_d\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    479\u001B[0m \u001B[43m                                                      \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    481\u001B[0m pred, true \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(x_test, y_test)\n\u001B[0;32m    482\u001B[0m metric_loss \u001B[38;5;241m=\u001B[39m math_sqrt(nn\u001B[38;5;241m.\u001B[39mMSELoss()(torch\u001B[38;5;241m.\u001B[39mtensor(pred), torch\u001B[38;5;241m.\u001B[39mtensor(true))\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[1;32m~\\Desktop\\Python\\TimeSeries\\es_load_fs_HUN\\models\\trainer_lib.py:813\u001B[0m, in \u001B[0;36mMIMOTSWrapper.train_strategy\u001B[1;34m(self, x_train, y_train, x_val, y_val, x_test, y_test, epochs, lr, optimizer, batch_size, loss_fn, es_p, es_d, verbose, cp, **kwargs)\u001B[0m\n\u001B[0;32m    810\u001B[0m     test_dataset: Dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_ts_dataset(x_test, y_test)\n\u001B[0;32m    811\u001B[0m     test_loader: DataLoader \u001B[38;5;241m=\u001B[39m DataLoader(test_dataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m--> 813\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    814\u001B[0m \u001B[43m                         \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mes_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mes_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mes_d\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mes_d\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    815\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcp\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\Python\\TimeSeries\\es_load_fs_HUN\\models\\trainer_lib.py:369\u001B[0m, in \u001B[0;36mTSMWrapper._train_model\u001B[1;34m(self, train_loader, val_loader, test_loader, epochs, lr, optimizer, loss_fn, es_p, es_d, verbose, cp)\u001B[0m\n\u001B[0;32m    367\u001B[0m early_stopper \u001B[38;5;241m=\u001B[39m EarlyStopper(patience\u001B[38;5;241m=\u001B[39mes_p, min_delta\u001B[38;5;241m=\u001B[39mes_d, model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m cp \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model)\n\u001B[0;32m    368\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[1;32m--> 369\u001B[0m     train_loss: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    370\u001B[0m     train_losses\u001B[38;5;241m.\u001B[39mappend(train_loss)\n\u001B[0;32m    372\u001B[0m     val_loss: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_test_model(val_loader, loss_fn\u001B[38;5;241m=\u001B[39mloss_fn)\n",
      "File \u001B[1;32m~\\Desktop\\Python\\TimeSeries\\es_load_fs_HUN\\models\\trainer_lib.py:931\u001B[0m, in \u001B[0;36mRECOneModelTSWrapper._train_epoch\u001B[1;34m(self, data_loader, lr, optimizer, loss_fn)\u001B[0m\n\u001B[0;32m    927\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    929\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m--> 931\u001B[0m features \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((features[:, \u001B[38;5;241m1\u001B[39m:], \u001B[43mlabels\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    932\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_teacher_forcing:\n\u001B[0;32m    933\u001B[0m     features[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mdetach()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "wrapper = tl.RECOneModelTSWrapper(GRURec(11, 70, 5, 0.5, 0.05, True), seq_len=24, pred_len=3)\n",
    "result = wrapper.validate_ts_strategy(X, y, 1000, batch_size=2048, lr=0.001, n_splits=6)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T09:56:04.172212800Z",
     "start_time": "2023-11-28T09:54:48.255836Z"
    }
   },
   "id": "fab42f2948e16e31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sum(result[3]) / len(result[3]), \"-\", sum(result[3][1:]) / (len(result[3]) - 1))\n",
    "st = X.shape[0] // 7\n",
    "tl.TSMWrapper.print_evaluation_info(*wrapper.predict(X[-st:], y[-st:]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47ed22314d48fbbb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
