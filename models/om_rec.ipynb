{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "174ec8f524d1b193"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import trainer_lib as tl\n",
    "import torch_model_definitions as tmd\n",
    "\n",
    "torch.manual_seed(310231551)\n",
    "random.seed(3009231410)\n",
    "np.random.seed(2909231846)\n",
    "np_random_state = np.random.RandomState(131002)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "587b63899b1298c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df: pd.DataFrame = tl.load_country_wide_dataset('../data/country_data.csv')\n",
    "\n",
    "X = df.to_numpy(dtype=np.float32)\n",
    "y = X.copy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b483975330fa7452"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1241fd1ff050dbf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LSTMRec(nn.Module):\n",
    "    def __init__(self, features=11, hidden_size=20, num_layers=2, dropout=0.0, hid_noise=0.0, bidirectional=True, **kwargs):\n",
    "        super(LSTMRec, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.h_n_dim = 2 if bidirectional else 1\n",
    "        self.num_layers = num_layers\n",
    "        rec_drop = dropout if num_layers > 1 else 0.0\n",
    "        self.lstm = nn.LSTM(input_size=features, hidden_size=self.hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, dropout=rec_drop)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            tmd.GaussianNoise(hid_noise),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden_size * self.h_n_dim * self.num_layers, features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h_0 = torch.zeros(self.h_n_dim * self.num_layers, batch_size, self.hidden_size).requires_grad_().to(tl.TRAINER_LIB_DEVICE)\n",
    "        c_0 = torch.zeros(self.h_n_dim * self.num_layers, batch_size, self.hidden_size).requires_grad_().to(tl.TRAINER_LIB_DEVICE)\n",
    "\n",
    "        _, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        h_n = torch.permute(h_n, (1, 0, 2))\n",
    "        return self.fc(h_n)\n",
    "    \n",
    "class GRURec(nn.Module):\n",
    "    def __init__(self, features=11, hidden_size=20, num_layers=2, dropout=0.0, hid_noise=0.0, bidirectional=True, **kwargs):\n",
    "        super(GRURec, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.h_n_dim = 2 if bidirectional else 1\n",
    "        self.num_layers = num_layers\n",
    "        rec_drop = dropout if num_layers > 1 else 0.0\n",
    "        self.gru = nn.GRU(input_size=features, hidden_size=self.hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, dropout=rec_drop)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            tmd.GaussianNoise(hid_noise),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.hidden_size * self.h_n_dim * self.num_layers, features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        hidden = torch.zeros(self.h_n_dim * self.num_layers, batch_size, self.hidden_size).requires_grad_().to(tl.TRAINER_LIB_DEVICE)\n",
    "        \n",
    "        _, hidden = self.gru(x, hidden)\n",
    "        x = torch.permute(hidden, (1, 0, 2))\n",
    "        return self.fc(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8e521f357fcaa3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grid search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "188b9f01ce6fa5b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid = tl.Grid({\n",
    "    'epochs': [1000],  # we use early stopping, so this is just a high number\n",
    "    'lr': [0.001],\n",
    "    'model': [LSTMRec, GRURec],\n",
    "    'hidden_size': [20, 40],\n",
    "    'num_layers': [2, 3],\n",
    "    'bidirectional': [True],\n",
    "    'batch_size': [2048],\n",
    "    'dropout': [0.5],\n",
    "    'hid_noise': [0.05],\n",
    "}) # val_mod is default at 8, n_splits at 6\n",
    "\n",
    "wrapper = tl.RECOneModelTSWrapper(LSTMRec(), seq_len=24, pred_len=3)\n",
    "b_p, b_s = wrapper.grid_search(X, y, grid, verbose=3)\n",
    "print(f\"\\nBest params: {b_p}\\nBest score: {b_s}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77d4f9ed316d4f71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The highest layer number and hidden size were the best, I will test even higher ones.\n",
    "LSTM and GRU trade blows in this comparison, further testing is required."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3480b94e7206252b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid = tl.Grid({\n",
    "    'epochs': [1000],  # we use early stopping, so this is just a high number\n",
    "    'lr': [0.001],\n",
    "    'model': [LSTMRec, GRURec],\n",
    "    'hidden_size': [60],\n",
    "    'num_layers': [3, 4],\n",
    "    'bidirectional': [True],\n",
    "    'batch_size': [2048],\n",
    "    'dropout': [0.5],\n",
    "    'hid_noise': [0.05],\n",
    "}) # val_mod is default at 8, n_splits at 6\n",
    "\n",
    "wrapper = tl.RECOneModelTSWrapper(LSTMRec(), seq_len=24, pred_len=3)\n",
    "b_p, b_s = wrapper.grid_search(X, y, grid, verbose=4)\n",
    "print(f\"\\nBest params: {b_p}\\nBest score: {b_s}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d993f4179a173103"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid = tl.Grid({\n",
    "    'epochs': [1000],  # we use early stopping, so this is just a high number\n",
    "    'lr': [0.0005],\n",
    "    'model': [GRURec],\n",
    "    'hidden_size': [50, 60, 70],\n",
    "    'num_layers': [4],\n",
    "    'bidirectional': [True],\n",
    "    'batch_size': [2048],\n",
    "    'dropout': [0.5],\n",
    "    'hid_noise': [0.05],\n",
    "}) # val_mod is default at 8, n_splits at 6\n",
    "\n",
    "wrapper = tl.RECOneModelTSWrapper(GRURec(), seq_len=24, pred_len=3)\n",
    "b_p, b_s = wrapper.grid_search(X, y, grid, verbose=3)\n",
    "print(f\"\\nBest params: {b_p}\\nBest score: {b_s}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ac240a89bb0551a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The GRU model with hidden size 70 and 4 layers seem to work the best."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5bce66cbffb6f7d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid = tl.Grid({\n",
    "    'epochs': [1000],  # we use early stopping, so this is just a high number\n",
    "    'lr': [0.001],\n",
    "    'model': [GRURec],\n",
    "    'hidden_size': [70, 80],\n",
    "    'num_layers': [5],\n",
    "    'bidirectional': [True],\n",
    "    'batch_size': [2048],\n",
    "    'dropout': [0.5],\n",
    "    'hid_noise': [0.05],\n",
    "}) # val_mod is default at 8, n_splits at 6\n",
    "\n",
    "wrapper = tl.RECOneModelTSWrapper(GRURec(), seq_len=24, pred_len=3)\n",
    "b_p, b_s = wrapper.grid_search(X, y, grid, verbose=3)\n",
    "print(f\"\\nBest params: {b_p}\\nBest score: {b_s}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "807a2f7e20ebf38b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Final"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e41b1d2e916b5e84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wrapper = tl.RECOneModelTSWrapper(GRURec(11, 70, 5, 0.5, 0.05, True), seq_len=24, pred_len=3)\n",
    "result = wrapper.validate_ts_strategy(X, y, 1000, batch_size=2048, lr=0.001, n_splits=6)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fab42f2948e16e31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sum(result[3]) / len(result[3]), \"-\", sum(result[3][1:]) / (len(result[3]) - 1))\n",
    "st = X.shape[0] // 7\n",
    "tl.TSMWrapper.print_evaluation_info(*wrapper.predict(X[-st:], y[-st:]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47ed22314d48fbbb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
